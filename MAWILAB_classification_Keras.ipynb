{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(383615, 32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./dataset/20170827_mawilab_flow_000.csv\",sep=',')\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>E</th>\n",
       "      <th>EC</th>\n",
       "      <th>F</th>\n",
       "      <th>PA</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  C  E  EC  F  PA  R  S\n",
       "0  0  0  0   0  0   0  0  0\n",
       "1  1  0  0   0  0   0  0  1\n",
       "2  1  0  0   0  0   0  0  0\n",
       "3  1  0  0   0  0   0  0  0\n",
       "4  1  0  0   0  0   0  0  0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'U'] = pd.Series(np.zeros(df.shape[0]),dtype='int8')\n",
    "df.loc[:, 'N'] = pd.Series(np.zeros(df.shape[0]),dtype='int8')\n",
    "new_flag_df = df['flags'].to_frame()\n",
    "new_flag_df = new_flag_df['flags'].str.get_dummies(' ')\n",
    "new_flag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   anomaly  normal\n",
      "0        0       1\n",
      "1        0       1\n",
      "2        0       1\n",
      "3        1       0\n",
      "4        1       0\n",
      "   pro  packets  bytes  svc_port  durat  U  N  A  C  E  EC  F  PA  R  S\n",
      "0    1        1     32         0    0.0  0  0  0  0  0   0  0   0  0  0\n",
      "1    6        1     48        80    0.0  0  0  1  0  0   0  0   0  0  1\n",
      "2    6        1   1440        22    0.0  0  0  1  0  0   0  0   0  0  0\n",
      "3    6        1     52        80    0.0  0  0  1  0  0   0  0   0  0  0\n",
      "4    6        1     52        80    0.0  0  0  1  0  0   0  0   0  0  0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Pro\n",
    "2. Packets\n",
    "3. Bytes\n",
    "4. Flags\n",
    "5. Durat\n",
    "6. Records\n",
    "7. svc_port - combination of sPort and dPort\n",
    "\n",
    "v = np.minimum(df['sPort'], df['dPort'])\n",
    "df['svc_port'] = np.where(v <= 1024, v, df['dPort'])\n",
    "\"\"\"\n",
    "df['svc_port'] = df.apply(lambda x : min(x['sPort'],x['dPort']) if min(x['sPort'],x['dPort']) <= 1024 else x['dPort'], axis=1)\n",
    "df = df.loc[:,('pro', 'packets', 'bytes', 'svc_port', 'durat','U','N','Records')]\n",
    "df.fillna(0, inplace=True) \n",
    "new_df = df.drop(['Records'], axis=1)\n",
    "mawii_labels = pd.get_dummies(df['Records'],prefix='',prefix_sep='')\n",
    "mawii_features = pd.concat([new_df, new_flag_df], axis=1)\n",
    "print(mawii_labels.head())\n",
    "print(mawii_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test set preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(383615, 15)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mawii_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `kernel_size` argument must be a tuple of 1 integers. Received: (5, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-749627a43819>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# ConvNet_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filters, kernel_size, strides, padding, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[0mkernel_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m             \u001b[0mbias_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m             **kwargs)\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'kernel_size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'strides'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_padding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36mnormalize_tuple\u001b[1;34m(value, n, name)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             raise ValueError('The `' + name + '` argument must be a tuple of ' +\n\u001b[1;32m---> 39\u001b[1;33m                              str(n) + ' integers. Received: ' + str(value))\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msingle_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue_tuple\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The `kernel_size` argument must be a tuple of 1 integers. Received: (5, 5)"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D,Dense, MaxPooling2D,Flatten\n",
    "import keras\n",
    "input_shape = mawii_features.shape\n",
    "num_classes = 2\n",
    "\n",
    "# ConvNet_1\n",
    "model = Sequential() \n",
    "model.add(Conv2D(64, kernel_size=(5, 5), strides=(1, 1),activation='relu',input_shape=(128,15,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_12_input to have 4 dimensions, but got array with shape (383615, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-31ddea2b531c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m           validation_data=(mawii_features, mawii_labels))\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1635\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1636\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1637\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1638\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1481\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1483\u001b[1;33m                                     exception_prefix='input')\n\u001b[0m\u001b[0;32m   1484\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1485\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    111\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    114\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_12_input to have 4 dimensions, but got array with shape (383615, 15)"
     ]
    }
   ],
   "source": [
    "model.fit(mawii_features, mawii_labels,\n",
    "          batch_size=128,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          validation_data=(mawii_features, mawii_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_2d(x,W): \n",
    "    # Padding : surround the original image with 0's \n",
    "    # valid padding - no padding.size of the image shrinks  resultant image : (n - f + 1, n - f + 1)\n",
    "    # same padding - resultant is same as that of original. resultant image : (n + 2p - f + 1, n +2p - f + 1)  p = (f - 1)/2\n",
    "    # Strides : ((n + 2p - f)/s + 1, (n +2p - f)/s + 1)\n",
    "    return tf.nn.conv2d(input = x, filter = W, strides = [1,1,1,1], padding=\"SAME\")\n",
    "\n",
    "def avg_pool_2x2(x):\n",
    "\n",
    "    # MAX/AVG Pooling : ((n + 2p - f)/s + 1, (n +2p - f)/s + 1)\n",
    "    return tf.nn.avg_pool(x, k_size=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn-1,2,3 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv1_network(x_image, reuse=False): \n",
    "    with tf.variable_scope('ConvNet1') as scope: \n",
    "        if reuse:\n",
    "             tf.get_variable_scope().reuse_variables()\n",
    "        # First Conv and Pooling layers\n",
    "        W_conv1 = tf.get_variable('c_wconv1',[5,5,1,8],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv1 = tf.get_variable('c_bconv1',[8],initializer=tf.constant_initializer(0))\n",
    "        h_conv1 = tf.nn.relu(conv_2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = avg_pool_2x2(h_conv1)\n",
    "        \n",
    "        # Fully Connected layer1 \n",
    "        # First fully connected layers\n",
    "        W_fc1 = tf.get_variable('d_wfc1', [7*7*8,16],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc1 = tf.get_variable('d_bfc1', [16], initializer=tf.constant_initializer(0))\n",
    "        h_pool_flat = tf.reshape(h_pool1, [-1,7*7*8])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool_flat,W_fc1)+b_fc1)\n",
    "        \n",
    "        #Seconf fully connected layers\n",
    "        W_fc2 = tf.get_variable('d_wfc2', [16,1],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc2 = tf.get_variable('d_bfc2', [1], initializer=tf.constant_initializer(0))\n",
    "\n",
    "        y_conv = tf.nn.relu(tf.matmul(h_fc1,W_fc2)+b_fc2)\n",
    "    \n",
    "    return y_conv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2_network(x_image, reuse=False): \n",
    "    with tf.variable_scope('ConvNet2') as scope: \n",
    "        if reuse:\n",
    "             tf.get_variable_scope().reuse_variables()\n",
    "        # First Conv and Pooling layers\n",
    "        W_conv1 = tf.get_variable('c_wconv1',[5,5,1,8],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv1 = tf.get_variable('c_bconv1',[8],initializer=tf.constant_initializer(0))\n",
    "        h_conv1 = tf.nn.relu(conv_2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = avg_pool_2x2(h_conv1)\n",
    "        \n",
    "        # Seond Conv and Pooling layers\n",
    "        W_conv2 = tf.get_variable('d_wconv2',[5,5,8,16],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv2 = tf.get_variable('d_bconv2',[16],initializer=tf.constant_initializer(0))\n",
    "        h_conv2 = tf.nn.relu(conv_2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = avg_pool_2x2(h_conv2)\n",
    "        \n",
    "        # First fully connected layers\n",
    "        W_fc1 = tf.get_variable('d_wfc1', [7*7*16,32],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc1 = tf.get_variable('d_bfc1', [32], initializer=tf.constant_initializer(0))\n",
    "        h_pool_flat = tf.reshape(h_pool2, [-1,7*7*16])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool_flat,W_fc1)+b_fc1)\n",
    "        \n",
    "        #Seconf fully connected layers\n",
    "        W_fc2 = tf.get_variable('d_wfc2', [32,1],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc2 = tf.get_variable('d_bfc2', [1], initializer=tf.constant_initializer(0))\n",
    "\n",
    "        y_conv = tf.nn.relu(tf.matmul(h_fc1,W_fc2)+b_fc2)\n",
    "    \n",
    "    return y_conv\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv3_network(x_image, reuse=False): \n",
    "    with tf.variable_scope('ConvNet3') as scope: \n",
    "        if reuse:\n",
    "             tf.get_variable_scope().reuse_variables()\n",
    "        # First Conv and Pooling layers\n",
    "        W_conv1 = tf.get_variable('c_wconv1',[5,5,1,8],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv1 = tf.get_variable('c_bconv1',[8],initializer=tf.constant_initializer(0))\n",
    "        h_conv1 = tf.nn.relu(conv_2d(x_image, W_conv1) + b_conv1)\n",
    "        h_pool1 = avg_pool_2x2(h_conv1)\n",
    "        \n",
    "        # Seond Conv and Pooling layers\n",
    "        W_conv2 = tf.get_variable('d_wconv2',[5,5,8,16],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv2 = tf.get_variable('d_bconv2',[16],initializer=tf.constant_initializer(0))\n",
    "        h_conv2 = tf.nn.relu(conv_2d(h_pool1, W_conv2) + b_conv2)\n",
    "        h_pool2 = avg_pool_2x2(h_conv2)\n",
    "        \n",
    "        # Third Conv and Pooling layers\n",
    "        W_conv3 = tf.get_variable('d_wconv2',[5,5,16,32],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_conv3 = tf.get_variable('d_bconv2',[32],initializer=tf.constant_initializer(0))\n",
    "        h_conv3 = tf.nn.relu(conv_2d(h_pool2, W_conv3) + b_conv3)\n",
    "        h_pool3 = avg_pool_2x2(h_conv3)\n",
    "        \n",
    "        # First fully connected layers\n",
    "        W_fc1 = tf.get_variable('d_wfc1', [7*7*32,64],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc1 = tf.get_variable('d_bfc1', [64], initializer=tf.constant_initializer(0))\n",
    "        h_pool_flat = tf.reshape(h_pool3, [-1,7*7*32])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool_flat,W_fc1)+b_fc1)\n",
    "        \n",
    "        #Seconf fully connected layers\n",
    "        W_fc2 = tf.get_variable('d_wfc2', [64,1],initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        b_fc2 = tf.get_variable('d_bfc2', [1], initializer=tf.constant_initializer(0))\n",
    "\n",
    "        y_conv = tf.nn.relu(tf.matmul(h_fc1,W_fc2)+b_fc2)\n",
    "    \n",
    "    return y_conv"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
